# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_core.ipynb.

# %% auto 0
__all__ = ['all_model_types', 'thinking_models', 'imagen_models', 'vertex_models', 'models', 'pricings', 'audio_token_pricings',
           'valid_func', 'get_repr', 'det_repr', 'contents', 'usage', 'get_pricing', 'is_youtube_url', 'mk_part',
           'mk_parts', 'mk_content', 'mk_contents', 'mk_msg', 'mk_msgs', 'goog_doc', 'prep_tool', 'f_result',
           'f_results', 'mk_fres_content', 'prep_tools', 'Client', 'Chat']

# %% ../nbs/00_core.ipynb 3
import os
import base64

import PIL
import mimetypes
import inspect

from typing import Union
from io import BytesIO
from urllib.parse import urlparse, parse_qs
from functools import wraps
from google import genai
from google.genai import types

from fastcore import imghdr
from fastcore.all import *
from fastcore.docments import *

from toolslm.funccall import call_func



# %% ../nbs/00_core.ipynb 4
all_model_types = {
    "gemini-2.0-flash-exp": "llm-imagen#gemini-2.0-flash",
    "gemini-2.0-flash-exp-image-generation": "llm-imagen#gemini-2.0-flash",
    "gemini-2.0-flash": "llm-vertex#gemini-2.0-flash",
    "gemini-2.0-flash-001": "llm-vertex#gemini-2.0-flash",
    "gemini-2.0-pro-exp-02-05": "llm#gemini-2.0-pro",
    "gemini-2.0-flash-lite": "llm#gemini-2.0-flash-lite",
    "gemini-1.5-flash": "llm-vertex#gemini-1.5-flash",
    "gemini-1.5-pro": "llm-vertex#gemini-1.5-pro",
    "gemini-1.5-pro-002": "llm-vertex#gemini-1.5-pro",
    "gemini-1.5-flash-8b": "llm#gemini-1.5-flash-8b",
    "gemini-2.0-flash-thinking-exp-01-21": "llm-thinking#gemini-2.0-flash-thinking",
    "imagen-3.0-generate-002": "imagen#imagen-3.0"
}

thinking_models = [m for m in all_model_types if "thinking" in all_model_types[m]]

imagen_models = [m for m in all_model_types if "imagen" in all_model_types[m]]

vertex_models = [m for m in all_model_types if "vertex" in all_model_types[m]]

models = [m for m in all_model_types if "llm" in all_model_types[m]]

models

# %% ../nbs/00_core.ipynb 40
def get_repr(m, lab=""):
    """Recurisvely fetch the markdown representation of genai.types fields, wrapping lists into `<details>` blocks"""
    if hasattr(m, '_repr_markdown_'): return m._repr_markdown_()
    if is_listy(m): return "\n".join([f"<details open='true'><summary>{lab}[{i}]</summary>{get_repr(li)}</details>" for i, li in enumerate(m)])
    if isinstance(m, dict): return "<ul>" + "\n".join([f"<li><b>{i}</b>: {get_repr(li, i)}</li>" for i, li in m.items()]) + "</ul>"
    if isinstance(m, bytes): return m[:10] + b'...'
    return str(m)


# %% ../nbs/00_core.ipynb 43
def det_repr(m): return "<ul>" + "".join(f"<li><code>{d}</code>: {get_repr(getattr(m, d), d)}</li>" for d in m.model_fields_set) + "</ul>"

# %% ../nbs/00_core.ipynb 46
@patch
def _repr_markdown_(self: genai._common.BaseModel):
    return det_repr(self)

# %% ../nbs/00_core.ipynb 50
def contents(r: genai.types.GenerateContentResponse):
    """Returns a dictionary with the contents of a Gemini model response"""
    cts = {'text': '', 'images': []}
    for part in nested_idx(r, 'candidates', 0, 'content', 'parts'):
        if part.text is not None: cts['text'] += part.text
        if part.inline_data is not None:
            cts['images'].append(types.Image(image_bytes=part.inline_data.data, mime_type=part.inline_data.mime_type))
    return cts
    

# %% ../nbs/00_core.ipynb 51
@patch
def _repr_markdown_(self: genai.types.GenerateContentResponse):
    c = None
    cts = contents(self)
    if cts['images'] or cts['text']:
        c = ''
        for img in cts['images']:
            b64 = base64.b64encode(img.image_bytes).decode("utf-8")
            c += f'<div style="width: 200px; height: auto;"><img src="data:{img.mime_type};base64,{b64}" /></div>'
        c += cts['text'].replace("\n", "<br />")
    else:
        calls = (f"<code>{call.name}({', '.join([f'{a}={v}' for a, v in call.args.items()])})</code>" for call in self.function_calls)
        calls_repr = '\n'.join(f'<li>{c}</li>' for c in calls)
        c = f"<ul>{calls_repr}</ul>"
    dets = det_repr(self)
    return f"""{c}\n<details>{dets}</details>"""

# %% ../nbs/00_core.ipynb 55
@patch(as_prop=True)
def html(self: types.Image):
    b64 = base64.b64encode(self.image_bytes).decode("utf-8")
    return f'<img src="data:{self.mime_type};base64,{b64}" />'


@patch
def _repr_markdown_(self: types.Image):
    return f"""<div style="width: 100px; height: auto;">{self.html}</div>

<details>
{det_repr(self)}
</details>"""

# %% ../nbs/00_core.ipynb 57
@patch(as_prop=True)
def img(self: types.GenerateImagesResponse):
    return self.generated_images[0].image._pil_image


@patch
def _repr_markdown_(self: types.GenerateImagesResponse):
    N = len(self.generated_images)
    cols = min(N, 4)
    rows = math.ceil(N / 4)
   
    ims = "".join([f"""<div style="display: grid; 
                    width: 100%; 
                    max-width: 1000px; 
                    height: auto;
                    margin: 0 auto; 
                    grid-template-columns: {rows}fr;
                    grid-template-rows: 1ft;
                   ">{gim.image.html}</div>""" 
                   for gim in self.generated_images])
    
    
    
    i = f"""
<div style="display: grid; 
                gap: 4px; 
                width: 100%;
                height: auto;
                max-width: 1000px; 
                margin: 0 auto; 
                padding: 4px;
                grid-template-columns: repeat({cols}, 1fr);
                grid-template-rows: repeat({rows}, 1fr);
                ">

{ims}

</div>
    """
    return f"""{i}

<details>
{det_repr(self)}
</details>
"""

# %% ../nbs/00_core.ipynb 61
def usage(inp=0,     # Number of input tokens (excluding cached)
          out=0,     # Number of output tokens
          cached=0): # Number of cached tokens
    """A quicker and simpler constructor for the Usage Metadata model"""
    return types.GenerateContentResponseUsageMetadata(cached_content_token_count=cached, 
                                                      candidates_token_count=out, 
                                                      prompt_token_count=inp + cached, 
                                                      total_token_count=inp + out + cached)

# %% ../nbs/00_core.ipynb 65
@patch(as_prop=True)
def cached(self: types.GenerateContentResponseUsageMetadata): 
    return self.cached_content_token_count or 0

@patch(as_prop=True)
def inp(self: types.GenerateContentResponseUsageMetadata): 
    return (self.prompt_token_count - self.cached) or 0

@patch(as_prop=True)
def out(self: types.GenerateContentResponseUsageMetadata): 
    return self.candidates_token_count or 0

@patch(as_prop=True)
def total(self: types.GenerateContentResponseUsageMetadata): 
    return self.total_token_count or self.prompt_token_count + self.candidates_token_count

# %% ../nbs/00_core.ipynb 68
@patch
def __repr__(self: types.GenerateContentResponseUsageMetadata):
    return f"Cached: {self.cached}; In: {self.inp}; Out: {self.out}; Total: {self.total}"

@patch
def _repr_markdown_(self: types.GenerateContentResponseUsageMetadata):
    return self.__repr__()

# %% ../nbs/00_core.ipynb 72
@patch
def __add__(self: types.GenerateContentResponseUsageMetadata, other):
    cached = getattr(self, "cached", 0) + getattr(other, "cached", 0)
    return usage(self.inp + other.inp, self.out + other.out, cached)

# %% ../nbs/00_core.ipynb 75
# $/1M input (non cached) tokens, $/1M output tokens, $/1M cached input tokens, 

pricings = {
    'gemini-2.0-flash': [0.1, 0.4, 0.025],
    'gemini-2.0-flash-lite': [0.075, 0.3, 0.01875],
    'gemini-1.5-flash_short': [0.075, 0.3, 0.01875],
    'gemini-1.5-flash_long': [0.15, 0.6, 0.0375], 
    'gemini-1.5-flash-8b_short': [0.0375, 0.15, 0.01],
    'gemini-1.5-flash-8b_long': [0.075, 0.3, 0.02],
    'gemini-1.5-pro_short': [1.25, 5., 0.3125],   
    'gemini-1.5-pro_long': [2.5, 10., 0.625],
 }


audio_token_pricings = {
    'gemini-2.0-flash': [0.7, 0.4, 0.175],
}

def get_pricing(model, prompt_tokens):
    if "exp" in model: return [0, 0, 0]
    suff = "_long" if prompt_tokens > 128_000 else "_short"
    m = all_model_types.get(model, "#").split("#")[-1]
    m += suff if "1.5" in m else ""
    return pricings.get(m, [0, 0, 0])


# %% ../nbs/00_core.ipynb 78
@patch(as_prop=True)
def cost(self: types.GenerateContentResponse):
    ip, op, cp = get_pricing(self.model_version, self.usage_metadata.prompt_token_count)
    return ((self.usage_metadata.inp * ip) + (self.usage_metadata.out * op) + (self.usage_metadata.cached * cp)) / 1e6

# %% ../nbs/00_core.ipynb 80
@patch(as_prop=True)
def cost(self: types.GenerateImagesResponse): return 0.03 * len(self.generated_images)

# %% ../nbs/00_core.ipynb 86
def is_youtube_url(url: str) -> bool:
    """Check if the given URL is a valid YouTube video URL using urllib for parsing."""
    parsed = urlparse(url)
    if parsed.scheme not in ('http', 'https'): return False
    host = parsed.netloc.lower()
    
    # Standard YouTube URL (e.g., https://www.youtube.com/watch?v=VIDEO_ID)
    if host in ('www.youtube.com', 'youtube.com', 'm.youtube.com'):
        if parsed.path == '/watch':
            query_params = parse_qs(parsed.query)
            if 'v' in query_params:
                video_id = query_params['v'][0]
                return len(video_id) == 11
            return False
        # Embedded YouTube URL (e.g., https://www.youtube.com/embed/VIDEO_ID)
        elif parsed.path.startswith('/embed/'):
            video_id = parsed.path.split('/embed/')[1]
            return len(video_id) == 11

    # Shortened YouTube URL (e.g., https://youtu.be/VIDEO_ID)
    elif host == 'youtu.be':
        video_id = parsed.path.lstrip('/')
        return len(video_id) == 11
    return False

# %% ../nbs/00_core.ipynb 88
def mk_part(inp: Union[str, Path, types.Part, types.File, PIL.Image.Image], c: genai.Client|None=None):
    "Turns an input fragment into a multimedia `Part` to be sent to a Gemini model"
    api_client = c or genai.Client(api_key=os.environ["GEMINI_API_KEY"])
    if isinstance(inp, types.Part): return inp
    if isinstance(inp, types.File): return types.Part(file_data={"file_uri": inp.uri, "mime_type": inp.mime_type})
    if isinstance(inp, PIL.Image.Image): return types.Part.from_bytes(data=inp.tobytes(), mime_type=inp.get_format_mimetype())
    if isinstance(inp, bytes):
        mt = mimetypes.types_map["." + imghdr.what(None, h=inp)]
        return types.Part.from_bytes(data=inp, mime_type=mt)
    p_inp = Path(inp)
    if p_inp.exists():
        mt = mimetypes.guess_type(p_inp)[0]
        if mt.split("/")[0] == "image": return types.Part.from_bytes(data=p_inp.read_bytes(), mime_type=mt)
        file = api_client.files.upload(file=p_inp)
        return mk_part(file, c)
    if is_youtube_url(inp): return types.Part.from_uri(file_uri=inp, mime_type='video/*')
    return types.Part.from_text(text=inp)
        

# %% ../nbs/00_core.ipynb 96
def mk_parts(inps, c=None):
    return list(L(inps).map(mk_part, c=c)) if inps else [""]

# %% ../nbs/00_core.ipynb 100
def mk_content(content, role='user', cli=None):
    if isinstance(content, types.Content): return content.model_copy(update={'role': content.role or role})
    if isinstance(content, dict): mk_content(types.Content.model_construct(types.ContentDict(**content)), role)
    c = cli or genai.Client(api_key=os.environ['GEMINI_API_KEY'])
    return types.Content(role=role, parts=mk_parts(content, c=c))

def _is_msg(item):
    if isinstance(item, (types.Content, list)): return True
    if isinstance(item, dict):
        try:
            types.ContentDict(**item)
            return True
        except: return False

def mk_contents(inps, cli=None):
    if not (is_listy(inps) and any(_is_msg(o) for o in inps)): return [mk_content(inps, cli=cli)]
    return [mk_content(o, ('user', 'model')[i % 2], cli) for i, o in enumerate(inps)]

# %% ../nbs/00_core.ipynb 107
def mk_msg(content: list | str | types.Content, role:str='user', *args, api='genai', **kw):
    """Create a `Content` object from the actual content (GenAI's equivalent of a Message)"""
    c = kw.get('client', genai.Client(api_key=os.environ['GEMINI_API_KEY']))
    return mk_content(content, role, cli=c)


def mk_msgs(msgs: list | str, *args, api:str="openai", **kw) -> list:
    "Create a list of messages compatible with the GenAI sdk"
    if isinstance(msgs, str): msgs = [msgs]
    return [mk_msg(o, ('user', 'model')[i % 2], *args, api=api, **kw) for i, o in enumerate(msgs)]

# %% ../nbs/00_core.ipynb 115
@patch
@delegates(genai.models.Models.__call__)
def __call__(self: genai.Client, inps=None, **kwargs):
    return self.models(inps, client=self, **kwargs)

# %% ../nbs/00_core.ipynb 118
@patch(as_prop=True)
def use(self: genai.models.Models): return getattr(self, "_u", usage())

@patch(as_prop=True)
def cost(self: genai.models.Models): return getattr(self, "_cost", 0)


@patch(as_prop=True)
def use(self: genai.Client): return self.models.use

@patch(as_prop=True)
def cost(self: genai.Client): return self.models.cost

# %% ../nbs/00_core.ipynb 126
@patch(as_prop=True)
def _parts(self: types.GenerateContentResponse): return nested_idx(self, "candidates", 0, "content", "parts") or []
    

@patch
def _stream(self: genai.models.Models, s):
    all_parts = []
    for r in s:
        all_parts.extend(r._parts)
        yield r.text
    r.candidates[0].content.parts = all_parts
    self._r(r)

# %% ../nbs/00_core.ipynb 137
def _googlify_docs(fdoc:str,                  # Docstring of a function
                    argdescs: dict|None=None, # Dict of arg:docment of the arguments of the function
                    retd: str|None=None       # Return docoment of the function
                   )-> str: # The function docstring following Google style guide
    """Turns a function docment and docstring into a docstrings that function """
    if argdescs: fdoc += "\n\nArgs:\n"  + "\n".join([f"    {p}: {desc}" for p, desc in argdescs.items()])
    if retd: fdoc += f"\n\nReturns:\n    {retd}"
    return fdoc

def goog_doc(f:callable # A docment style function
            )->str:     # Google style docstring
    """Builds the docstring for a docment style function following Google style guide"""
    fdoc = f.__doc__
    args = {par: doc for par, doc in docments(f, returns=False).items() if doc is not None}
    retd = docments(f, full=True, returns=True)['return']['docment']
    return _googlify_docs(fdoc, args, retd)
    

# %% ../nbs/00_core.ipynb 140
def _geminify(f: callable) -> callable:
    """Makes a function suitable to be turned into a function declaration: 
    infers argument types from default values and removes the values from the signature"""
    docs = docments(f, full=True)
    new_params = [inspect.Parameter(name=n,
                                    kind=inspect.Parameter.POSITIONAL_OR_KEYWORD,
                                    annotation=i.anno) for n, i in docs.items() if n != 'return']

        
    @wraps(f)
    def wrapper(*args, **kwargs):
        return f(*args, **kwargs)
    
    wrapper.__signature__ = inspect.Signature(new_params, return_annotation=docs['return']['anno'])
    wrapper.__annotations__ = {n: i['anno'] for n, i in docs.items() if n != 'return'}
    return wrapper


def prep_tool(f:callable, # The function to be passed to the LLM
             as_decl:bool=False,  # Return an enriched genai.types.FunctionDeclaration?
             googlify_docstring:bool=True): # Use docments to rewrite the docstring following Google Style Guide? 
    """Optimizes a dunction for function calling with the Gemini api. Best suited for docments style functions."""
    docs = goog_doc(f) if googlify_docstring else f.__doc__
    _f = _geminify(f)
    _f.__doc__ = docs
    if not as_decl: return _f
    f_decl = types.FunctionDeclaration.from_callable_with_api_option(callable=_f, api_option='GEMINI_API')
    for par, desc in docments(f, returns=False).items():
        if desc: f_decl.parameters.properties[par].description = desc
    required_params = [p for p, d in docments(f, full=True, returns=False).items() if d['default'] == inspect._empty]
    f_decl.parameters.required = required_params
    return f_decl

# %% ../nbs/00_core.ipynb 145
def f_result(fname, fargs, ns=None):
    try: return {"result": call_func(fname, fargs, ns)}
    except Exception as e: return {'error': str(e)}

def f_results(fcalls, ns=None):
    return [{"name": c.name, "response": f_result(c.name, c.args, ns)} for c in fcalls]

def mk_fres_content(fres):
    return types.Content(role='tool', parts=[types.Part.from_function_response(**d) for d in fres])

# %% ../nbs/00_core.ipynb 148
@patch
def _r(self: genai.models.Models, r):
    self.result = [nested_idx(r, "candidates", 0, "content")]
    self._u = self.use + getattr(r, "usage_metadata", usage())
    self._cost = self.cost + r.cost
    for func in getattr(self, 'post_cbs', []): func(r)
    if r.function_calls:
        self.result.append(mk_fres_content(f_results(r.function_calls, ns=getattr(self, "_tools", None))))
    return r

   

# %% ../nbs/00_core.ipynb 150
def prep_tools(tools, toolify_everything=False):
    funcs = [prep_tool(f, as_decl=toolify_everything) for f in tools if inspect.isfunction(f)]
    if toolify_everything: funcs = [types.Tool(function_declarations=[f]) for f in funcs]
    tools_ = [t for t in tools if isinstance(t, types.Tool)]
    class_tools = [types.Tool(function_declarations=[prep_tool(f, as_decl=True)]) for f in tools if inspect.isclass(f)]
    return funcs + tools_ + class_tools

# %% ../nbs/00_core.ipynb 165
@patch
def structured(self: genai.models.Models, inps, tool, model=None):
    _ = self(inps, tools=[tool], model=model, use_afc=False, tool_mode="ANY")  
    return [nested_idx(ct, "function_response", "response", "result") for ct in nested_idx(self, "result", -1, "parts") or []]

@patch
def structured(self: genai.Client, inps, tool, model=None):
    return self.models.structured(inps, tool, model)

# %% ../nbs/00_core.ipynb 172
@patch
def _genconf(self: genai.models.Models, **kw):
    """Builds a GenerateContentConfigDict from call parameters"""
    config= {k: v for k, v in kw.items() if k in types.GenerateContentConfigDict.__annotations__}
    if _sp := kw.get("sp", False) or kw.get('system_instruction', False) or getattr(self, 'sp', False):
        config['system_instruction'] = _sp 
    if _temp := kw.get("temp", False) or kw.get('temperature', False) or getattr(self, 'temp', False):
        config['temperature'] = _temp
    if maxtok := kw.get("maxtok", False): config['max_output_tokens'] = maxtok
    if stop := kw.get("stop", False): config['stop_sequences'] = [stop] if isinstance(stop, str) else stop

    if tools:= kw.get("tools", False):
        self._tools = tools
        config['tools'] = prep_tools(tools, toolify_everything=not kw.get("use_afc", True))
        tc = config.get('tool_config', dict())
        fcc = tc.get('function_calling_config', dict())
        fcc['mode'] = kw.get("tool_mode", 'AUTO')
        tc['function_calling_config'] = fcc
        config['tool_config']= tc
        
    if model := kw.get('model', None) in imagen_models and not getattr(self, "text_only", False):
        config['response_modalities'] = kw.get('response_modalities', ['Text', 'Image'])
        
    return config
    

@patch
def __call__(self: genai.models.Models, 
             inps=None, # The inputs to be passed to the model
             sp:str='', # Optional system prompt
             temp:float=0.6, # Temperature
             maxtok:int|None=None, # Maximum number of tokens for the output
             stream:bool=False, # Stream response?
             stop:str|list[str]|None=None, # Stop sequence[s]
             tools=None, # A list of functions or tools to be passed to the model
             use_afc=True, # Use Google's automatic function calling? If False, functions will be converted to tools
             # `AUTO` lets the model decide whether tools to use, 
             # `ANY` forces the model to call a function `NONE` avoids any function calling
             tool_mode='AUTO',  
             **kwargs):
    """Call to a Gemini LLM"""
    kwargs["model"] = kwargs.get("model", getattr(self, "model", None))
    model = kwargs["model"]
    config = self._genconf(sp=sp, temp=temp, maxtok=maxtok, stop=stop, tools=tools, use_afc=use_afc, 
                           tool_mode=tool_mode, **kwargs)
    
    contents = mk_contents(inps, cli=kwargs.get('client', None))    
    gen_f = self.generate_content_stream if stream else self.generate_content
    r = gen_f(model=model, contents=contents, config=config if config else None)
    return self._stream(r) if stream else self._r(r)


# %% ../nbs/00_core.ipynb 179
def Client(model:str, # The model to be used by default (can be overridden when generating)
           sp:str='', # System prompt
           temp:float=0.6, # Default temperature
           text_only:bool=False, # Suppress multimodality even if the model allows for it
          ): 
    """An extension of `google.genai.Client` with a series of quality of life improvements"""
    c = genai.Client(api_key=os.environ['GEMINI_API_KEY'])
    c.models.model, c.models.sp, c.models.temp, c.models.text_only = model, sp, temp, text_only
    return c

# %% ../nbs/00_core.ipynb 187
@patch
def imagen(self: genai.models.Models,
           prompt:str, # Prompt for the image to be generated
           n_img:int=1): # Number of images to be generated (1-8)
    """Generate one or more images using the latest Imagen model."""
    return self.generate_images(
                model = 'imagen-3.0-generate-002',
                prompt=prompt, config={"number_of_images": n_img})

@patch
@delegates(to=genai.models.Models)
def imagen(self: genai.Client, prompt, **kwargs):
    return self.models.imagen(prompt, **kwargs)

# %% ../nbs/00_core.ipynb 190
valid_func = genai.chats._validate_response

@patch(as_prop=True)
def client(self: genai.chats.Chat): return self._modules

@patch(as_prop=True)
def h(self: genai.chats.Chat): return self._curated_history

@patch(as_prop=True)
def full_h(self: genai.chats.Chat): return self._comprehensive_history

@patch
def _rec_res(self: genai.chats.Chat, resp):
    if not getattr(self, "user_query", False): return
    resp_c = nested_idx(resp, "candidates", 0, "content")
    self.record_history(
        user_input=self.user_query,
        model_output=[resp_c] if resp_c else [],
        automatic_function_calling_history=resp.automatic_function_calling_history,
        is_valid=valid_func(resp)
    )

def Chat(model:str, # The model to be used 
           sp:str='', # System prompt
           temp:float=0.6, # Default temperature
           text_only:bool=False, # Suppress multimodality even if the model allows for it
           cli:genai.Client|None=None, # Optional Client to be passed (to keep track of usage)
          ): 
    """An extension of `google.genai.chats.Chat` with a series of quality of life improvements"""        
    c = Client(model, sp, temp, text_only) if cli is None else cli
    c.model, c.sp, c.temp, c.text_only = model, sp, temp, text_only
    chat = c.chats.create(model=c.model)
    chat.client.post_cbs = [chat._rec_res]
    return chat

# %% ../nbs/00_core.ipynb 191
@patch
@delegates(genai.Client.__call__, keep=True)
def __call__(self: genai.chats.Chat, inps=None, **kwargs):
    self.user_query = mk_content(inps) if inps else self.client.result[-1]
    return self.client(self.h + [self.user_query], **kwargs)

