{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95f5802f-f203-4455-9de6-f72f43040aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp toolloop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d3ef65-b77e-4937-b78f-70fc3b40c61f",
   "metadata": {},
   "source": [
    "# Tool Loop\n",
    "\n",
    "The code for Claudette's tool loop should essentially work as is. We'll replicate the whole original notebook just to make sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8eb7ee4-91a4-4f58-942a-ecd3d78eb1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from gaspare.core import *\n",
    "from fastcore.utils import *\n",
    "from fastcore.meta import delegates\n",
    "\n",
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2960b1d2-ddb9-49f8-8665-3d89af95848d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gemini-2.0-flash-exp',\n",
       " 'gemini-2.0-flash-exp-image-generation',\n",
       " 'gemini-2.0-flash',\n",
       " 'gemini-2.0-flash-001',\n",
       " 'gemini-2.0-pro-exp-02-05',\n",
       " 'gemini-2.0-flash-lite',\n",
       " 'gemini-1.5-flash',\n",
       " 'gemini-1.5-pro',\n",
       " 'gemini-1.5-pro-002',\n",
       " 'gemini-1.5-flash-8b',\n",
       " 'gemini-2.0-flash-thinking-exp-01-21']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8f9f6dd6-1216-4776-90f2-8910fc883cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemini-2.0-flash-lite'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = 'gemini-2.0-flash-lite'\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cb0680-f3bd-4fba-83b5-4d72ad22e988",
   "metadata": {},
   "source": [
    "Let's use the [same example](https://github.com/anthropics/anthropic-cookbook/blob/main/tool_use/customer_service_agent.ipynb) from Claudette's documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c36025e7-18aa-48ad-a313-0badce6ea107",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = {\n",
    "    \"O1\": dict(id=\"O1\", product=\"Widget A\", quantity=2, price=19.99, status=\"Shipped\"),\n",
    "    \"O2\": dict(id=\"O2\", product=\"Gadget B\", quantity=1, price=49.99, status=\"Processing\"),\n",
    "    \"O3\": dict(id=\"O3\", product=\"Gadget B\", quantity=2, price=49.99, status=\"Shipped\")}\n",
    "\n",
    "customers = {\n",
    "    \"C1\": dict(name=\"John Doe\", email=\"john@example.com\", phone=\"123-456-7890\",\n",
    "               orders=[orders['O1'], orders['O2']]),\n",
    "    \"C2\": dict(name=\"Jane Smith\", email=\"jane@example.com\", phone=\"987-654-3210\",\n",
    "               orders=[orders['O3']])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8887b19-fd70-4d9f-9089-8278445f6a34",
   "metadata": {},
   "source": [
    "As with Claudette, we do not have to create the JSON schema manually. We can use docments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b544f2ad-0e09-4466-9ae2-ada005227996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_customer_info(\n",
    "    customer_id:str # ID of the customer\n",
    "): # Customer's name, email, phone number, and list of orders\n",
    "    \"Retrieves a customer's information and their orders based on the customer ID\"\n",
    "    print(f'- Retrieving customer {customer_id}')\n",
    "    return customers.get(customer_id, \"Customer not found\")\n",
    "\n",
    "def get_order_details(\n",
    "    order_id:str # ID of the order\n",
    "): # Order's ID, product name, quantity, price, and order status\n",
    "    \"Retrieves the details of a specific order based on the order ID\"\n",
    "    print(f'- Retrieving order {order_id}')\n",
    "    return orders.get(order_id, \"Order not found\")\n",
    "\n",
    "def cancel_order(\n",
    "    order_id:str # ID of the order to cancel\n",
    ")->bool: # True if the cancellation is successful\n",
    "    \"Cancels an order based on the provided order ID\"\n",
    "    print(f'- Cancelling order {order_id}')\n",
    "    if order_id not in orders: return False\n",
    "    orders[order_id]['status'] = 'Cancelled'\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a7fd0a-93b0-4c76-b8c1-1b40025642e7",
   "metadata": {},
   "source": [
    "We are ready to go. The main difference here is that we don't assign the tools to the chat itself, since otherwise Gemini becomes too eager to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "45ddf3f7-a1c7-4178-bf34-03c9ac2345b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_customer_info, get_order_details, cancel_order]\n",
    "chat = Chat(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ac4935c5-4504-4e0d-ba22-4879024e5703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Retrieving customer C1\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<ul><li><code>get_customer_info(customer_id=C1)</code></li></ul>\n",
       "<details><ul><li><code>usage_metadata</code>: Cached: 0; In: 410; Out: 20; Total: 430</li><li><code>model_version</code>: gemini-2.0-pro-exp-02-05</li><li><code>candidates</code>: <details open='true'><summary>candidates[0]</summary><ul><li><code>index</code>: 0</li><li><code>finish_reason</code>: FinishReason.STOP</li><li><code>content</code>: <ul><li><code>parts</code>: <details open='true'><summary>parts[0]</summary><ul><li><code>function_call</code>: <ul><li><code>args</code>: <ul><li><b>customer_id</b>: C1</li></ul></li><li><code>name</code>: get_customer_info</li></ul></li></ul></details></li><li><code>role</code>: model</li></ul></li></ul></details></li><li><code>automatic_function_calling_history</code>: </li></ul></details>"
      ],
      "text/plain": [
       "GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'customer_id': 'C1'}, name='get_customer_info'), function_response=None, inline_data=None, text=None)], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=None, finish_reason=<FinishReason.STOP: 'STOP'>, grounding_metadata=None, index=0, logprobs_result=None, safety_ratings=None)], create_time=None, response_id=None, model_version='gemini-2.0-pro-exp-02-05', prompt_feedback=None, usage_metadata=Cached: 0; In: 410; Out: 20; Total: 430, automatic_function_calling_history=[], parsed=None)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = 'Can you tell me the email address for customer C1?'\n",
    "pr = 'Tell me the email address for customer C1.'\n",
    "r = chat(pr, tools=tools, use_afc=False)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8042fc83-48b5-4d8a-94f8-f17623088619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The email address for customer C1 is john@example.com.<br />\n",
       "<details><ul><li><code>usage_metadata</code>: Cached: 0; In: 332; Out: 14; Total: 346</li><li><code>model_version</code>: gemini-2.0-pro-exp-02-05</li><li><code>candidates</code>: <details open='true'><summary>candidates[0]</summary><ul><li><code>index</code>: 0</li><li><code>finish_reason</code>: FinishReason.STOP</li><li><code>content</code>: <ul><li><code>parts</code>: <details open='true'><summary>parts[0]</summary><ul><li><code>text</code>: The email address for customer C1 is john@example.com.\n",
       "</li></ul></details></li><li><code>role</code>: model</li></ul></li></ul></details></li><li><code>automatic_function_calling_history</code>: </li></ul></details>"
      ],
      "text/plain": [
       "GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='The email address for customer C1 is john@example.com.\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=None, finish_reason=<FinishReason.STOP: 'STOP'>, grounding_metadata=None, index=0, logprobs_result=None, safety_ratings=None)], create_time=None, response_id=None, model_version='gemini-2.0-pro-exp-02-05', prompt_feedback=None, usage_metadata=Cached: 0; In: 332; Out: 14; Total: 346, automatic_function_calling_history=[], parsed=None)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c9d35f5c-2842-4411-95f7-cd5dd66e7264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I am unable to fulfill this request. I can only cancel orders one at a time, and I need the order ID to do so.<br />\n",
       "<details><ul><li><code>usage_metadata</code>: Cached: 0; In: 209; Out: 29; Total: 238</li><li><code>model_version</code>: gemini-2.0-flash-lite</li><li><code>candidates</code>: <details open='true'><summary>candidates[0]</summary><ul><li><code>finish_reason</code>: FinishReason.STOP</li><li><code>content</code>: <ul><li><code>parts</code>: <details open='true'><summary>parts[0]</summary><ul><li><code>text</code>: I am unable to fulfill this request. I can only cancel orders one at a time, and I need the order ID to do so.\n",
       "</li></ul></details></li><li><code>role</code>: model</li></ul></li><li><code>avg_logprobs</code>: -0.1087644100189209</li></ul></details></li><li><code>automatic_function_calling_history</code>: </li></ul></details>"
      ],
      "text/plain": [
       "GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='I am unable to fulfill this request. I can only cancel orders one at a time, and I need the order ID to do so.\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=-0.1087644100189209, finish_reason=<FinishReason.STOP: 'STOP'>, grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], create_time=None, response_id=None, model_version='gemini-2.0-flash-lite', prompt_feedback=None, usage_metadata=Cached: 0; In: 209; Out: 29; Total: 238, automatic_function_calling_history=[], parsed=None)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = \"\"\"You will be provided with tools, but don't limit your answer to those tools.\n",
    "If the user query is related to some of the tools you have access to come up with a sequence of actions to achieve the goal and **execute the plan immediately**.\n",
    "\n",
    "If the user query is unrelated to the tools you have access to, answer the query using your own knowledge.\"\"\"\n",
    "\n",
    "chat = Chat(model)\n",
    "r = chat('Cancel all orders for customer C1.', tools=tools, use_afc=False, sp=sp, temp=0.6)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0c44ad55-b87e-463f-b924-56ae2e35315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "@patch\n",
    "@delegates(genai.chats.Chat.__call__)\n",
    "def toolloop(self:genai.chats.Chat,\n",
    "             pr, # Prompt to pass to Gemini\n",
    "             max_steps=10, # Maximum number of tool requests to loop through\n",
    "             trace_func:Optional[callable]=None, # Function to trace tool use steps (e.g `print`)\n",
    "             cont_func:Optional[callable]=noop, # Function that stops loop if returns False\n",
    "             **kwargs):\n",
    "    \"Add prompt `pr` to dialog and get a response from Gemini, automatically following up with `tool_use` messages\"\n",
    "    n_msgs = len(self.h)\n",
    "    kwargs[\"use_afc\"] = False\n",
    "    r = self(pr, **kwargs)\n",
    "    for i in range(max_steps):\n",
    "        if not r.function_calls:break\n",
    "        if trace_func: trace_func(self.h[n_msgs:]); n_msgs = len(self.h)\n",
    "        r = self(**kwargs)\n",
    "        if not (cont_func or noop)(self.h[-2]): break\n",
    "    if trace_func: trace_func(self.h[n_msgs:])\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6d1f588c-a5e0-469e-a1a7-47bbde5b637f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Retrieving customer C1\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The email address for customer C1 is john@example.com.<br />\n",
       "<details><ul><li><code>usage_metadata</code>: Cached: 0; In: 459; Out: 14; Total: 473</li><li><code>model_version</code>: gemini-2.0-pro-exp-02-05</li><li><code>candidates</code>: <details open='true'><summary>candidates[0]</summary><ul><li><code>index</code>: 0</li><li><code>finish_reason</code>: FinishReason.STOP</li><li><code>content</code>: <ul><li><code>parts</code>: <details open='true'><summary>parts[0]</summary><ul><li><code>text</code>: The email address for customer C1 is john@example.com.\n",
       "</li></ul></details></li><li><code>role</code>: model</li></ul></li></ul></details></li><li><code>automatic_function_calling_history</code>: </li></ul></details>"
      ],
      "text/plain": [
       "GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='The email address for customer C1 is john@example.com.\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=None, finish_reason=<FinishReason.STOP: 'STOP'>, grounding_metadata=None, index=0, logprobs_result=None, safety_ratings=None)], create_time=None, response_id=None, model_version='gemini-2.0-pro-exp-02-05', prompt_feedback=None, usage_metadata=Cached: 0; In: 459; Out: 14; Total: 473, automatic_function_calling_history=[], parsed=None)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat(model)\n",
    "r = chat.toolloop('Tell me the email address for customer C1.', tools=tools, sp=sp, temp=0.)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cfec47d8-9832-4cfa-acb5-98d86474a19f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/envs/prototypes/lib/python3.12/site-packages/IPython/core/formatters.py:406\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    404\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Repos/gaspare/gaspare/core.py:91\u001b[0m, in \u001b[0;36m_repr_markdown_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;129m@patch\u001b[39m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_repr_markdown_\u001b[39m(\u001b[38;5;28mself\u001b[39m: genai\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mGenerateContentResponse):\n\u001b[1;32m     90\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m     cts \u001b[38;5;241m=\u001b[39m \u001b[43mcontents\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m cts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     93\u001b[0m         c \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/Repos/gaspare/gaspare/core.py:80\u001b[0m, in \u001b[0;36mcontents\u001b[0;34m(r)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a dictionary with the contents of a Gemini model response\"\"\"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m cts \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m: []}\n\u001b[0;32m---> 80\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnested_idx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcandidates\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GenerateContentResponse(candidates=[Candidate(content=Content(parts=None, role=None), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=None, finish_reason=<FinishReason.MALFORMED_FUNCTION_CALL: 'MALFORMED_FUNCTION_CALL'>, grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], create_time=None, response_id=None, model_version='gemini-1.5-flash-8b-001', prompt_feedback=None, usage_metadata=Cached: 0; In: 209; Out: 0; Total: 209, automatic_function_calling_history=[], parsed=None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_msgs(msgs):\n",
    "    for n, m in enumerate(msgs):\n",
    "        for i, part in enumerate(m.parts):\n",
    "            print(f\"\\nMessage {n+1}, Part {i + 1}:\\n\")\n",
    "            c = \"* Text *: \" + part.text if part.text  else \"\" \n",
    "            c += \"* Function Call *: \" + str(part.function_call) if part.function_call else \"\"\n",
    "            c += \"* Function Response *: \" + str(part.function_response.response['result']) if part.function_response else \"\"\n",
    "            print(c)\n",
    "            print()\n",
    "\n",
    "chat = Chat(model)\n",
    "r = chat.toolloop('Cancel all orders for customer C1.', tools=tools, trace_func=print_msgs, temp=0., sp=sp)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e849776-9d40-4efc-90c1-f76670b0da69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Retrieving order O1\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The order status is Shipped.<br />\n",
       "<details><ul><li><code>usage_metadata</code>: Cached: 0; In: 161; Out: 7; Total: 168</li><li><code>model_version</code>: gemini-1.5-flash-8b-001</li><li><code>candidates</code>: <details open='true'><summary>candidates[0]</summary><ul><li><code>safety_ratings</code>: <details open='true'><summary>safety_ratings[0]</summary><ul><li><code>probability</code>: HarmProbability.NEGLIGIBLE</li><li><code>category</code>: HarmCategory.HARM_CATEGORY_HATE_SPEECH</li></ul></details>\n",
       "<details open='true'><summary>safety_ratings[1]</summary><ul><li><code>probability</code>: HarmProbability.NEGLIGIBLE</li><li><code>category</code>: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT</li></ul></details>\n",
       "<details open='true'><summary>safety_ratings[2]</summary><ul><li><code>probability</code>: HarmProbability.NEGLIGIBLE</li><li><code>category</code>: HarmCategory.HARM_CATEGORY_HARASSMENT</li></ul></details>\n",
       "<details open='true'><summary>safety_ratings[3]</summary><ul><li><code>probability</code>: HarmProbability.NEGLIGIBLE</li><li><code>category</code>: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT</li></ul></details></li><li><code>finish_reason</code>: FinishReason.STOP</li><li><code>content</code>: <ul><li><code>parts</code>: <details open='true'><summary>parts[0]</summary><ul><li><code>text</code>: The order status is Shipped.\n",
       "</li></ul></details></li><li><code>role</code>: model</li></ul></li><li><code>avg_logprobs</code>: -0.09802218845912389</li></ul></details></li><li><code>automatic_function_calling_history</code>: </li></ul></details>"
      ],
      "text/plain": [
       "GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='The order status is Shipped.\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=-0.09802218845912389, finish_reason=<FinishReason.STOP: 'STOP'>, grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=[SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=None, severity=None, severity_score=None)])], create_time=None, response_id=None, model_version='gemini-1.5-flash-8b-001', prompt_feedback=None, usage_metadata=Cached: 0; In: 161; Out: 7; Total: 168, automatic_function_calling_history=[], parsed=None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.toolloop('What is the status of order O1?', tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d76978-41aa-49ca-90fa-876ceda1b125",
   "metadata": {},
   "source": [
    "## Code interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57d566a6-7d9d-44fd-b446-e2c947c892b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolslm.shell import get_shell\n",
    "from fastcore.meta import delegates\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f52c860a-0f03-40e9-83aa-55fa5019e4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "imps = 'os, warnings, time, json, re, math, collections, itertools, functools, dateutil, datetime, string, types, copy, pprint, enum, numbers, decimal, fractions, random, operator, typing, dataclasses'\n",
    "\n",
    "def CodeChat(model: Optional[str] = None, ask:bool=True, tools=None, **kwargs):\n",
    "    imps = 'os, warnings, time, json, re, math, collections, itertools, functools, dateutil, datetime, string, types, copy, pprint, enum, numbers, decimal, fractions, random, operator, typing, dataclasses'\n",
    "    chat = Chat(model=model, **kwargs)\n",
    "    chat.ask = ask\n",
    "    chat.shell = get_shell()\n",
    "    chat.shell.run_cell('import '+ imps)\n",
    "    chat._tools = tools\n",
    "    chat._tools.append(chat.run_code)\n",
    "    return chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83d33598-3ba8-4f30-a2e2-f3a73e706366",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def run_code(\n",
    "    self:genai.chats.Chat,\n",
    "    code:str,   # Code to execute in persistent IPython session\n",
    "): # Result of expression on last line (if exists); '#DECLINED#' if user declines request to execute\n",
    "    \"Executes python code using a persistent IPython session. It asks the user for permission before executing the code.\"\n",
    "    confirm = f'Press Enter to execute, or enter \"n\" to skip?\\n```\\n{code}\\n```\\n'\n",
    "    if input(confirm): return '#DECLINED#'\n",
    "    try: res = self.shell.run_cell(code)\n",
    "    except Exception as e: return traceback.format_exc()\n",
    "    return res.stdout if res.result is None else res.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f07da44a-96b0-4257-a864-7e3d94f3d361",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = f'''You are a knowledgable coding assistant assistant. \n",
    "Don't do complex calculations yourself -- create code for them. \n",
    "Whenever you create code, run it in the IPython environment using the `run_code` tool.\n",
    "The following modules are pre-imported for `run_code` automatically:\n",
    "\n",
    "{imps}\n",
    "\n",
    "Note that `run_code` interpreter state is *persistent* across calls. \n",
    "\n",
    "If a tool returns `#DECLINED#` report to the user that the attempt was declined and no further progress can be made.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a6a5715-1af3-4403-b0d3-71e60aa424a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user(\n",
    "            ) -> str: # Username of current user\n",
    "    \"\"\"Get the username of the user running this session\"\"\"\n",
    "    print(\"Looking up username\")\n",
    "    return 'Miko'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f077806-0622-4c6d-bc0b-6b29f93a39e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = CodeChat('gemini-2.0-flash-lite', tools=[get_user], sp=sp, ask=True, temp=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8924bc0-8bd9-4dcc-87fb-1c8aa90556eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to execute, or enter \"n\" to skip?\n",
      "```\n",
      "\n",
      "from functools import reduce\n",
      "\n",
      "def checksum(s: str) -> int:\n",
      "    return reduce(lambda x, y: x * ord(y), s, 1)\n",
      "\n",
      "```\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```python<br />from functools import reduce<br /><br />def checksum(s: str) -> int:<br />    return reduce(lambda x, y: x * ord(y), s, 1)<br />```\n",
       "<details><ul><li><code>usage_metadata</code>: Cached: 0; In: 310; Out: 41; Total: 351</li><li><code>model_version</code>: gemini-2.0-flash-lite</li><li><code>candidates</code>: <details open='true'><summary>candidates[0]</summary><ul><li><code>finish_reason</code>: FinishReason.STOP</li><li><code>content</code>: <ul><li><code>parts</code>: <details open='true'><summary>parts[0]</summary><ul><li><code>text</code>: ```python\n",
       "from functools import reduce\n",
       "\n",
       "def checksum(s: str) -> int:\n",
       "    return reduce(lambda x, y: x * ord(y), s, 1)\n",
       "```</li></ul></details></li><li><code>role</code>: model</li></ul></li><li><code>avg_logprobs</code>: -0.050492170380383006</li></ul></details></li><li><code>automatic_function_calling_history</code>: </li></ul></details>"
      ],
      "text/plain": [
       "GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='```python\\nfrom functools import reduce\\n\\ndef checksum(s: str) -> int:\\n    return reduce(lambda x, y: x * ord(y), s, 1)\\n```')], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=-0.050492170380383006, finish_reason=<FinishReason.STOP: 'STOP'>, grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], create_time=None, response_id=None, model_version='gemini-2.0-flash-lite', prompt_feedback=None, usage_metadata=Cached: 0; In: 310; Out: 41; Total: 351, automatic_function_calling_history=[], parsed=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = '''Create a 1-line function `checksum` for a string `s`, that multiplies together the ascii \n",
    "values of each character in `s` using `reduce`.'''\n",
    "r = chat.toolloop(pr, tools=chat._tools, sp=sp, temp=0.6)\n",
    "r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3dd28f47-545f-4d02-b19c-882a0f723d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking up username\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to execute, or enter \"n\" to skip?\n",
      "```\n",
      "from functools import reduce\n",
      "\n",
      "def checksum(s: str) -> int:\n",
      "    return reduce(lambda x, y: x * ord(y), s, 1)\n",
      "\n",
      "print(checksum('Miko'))\n",
      "```\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The checksum of the username \"Miko\" is 96025545.<br />\n",
       "<details><ul><li><code>usage_metadata</code>: Cached: 0; In: 511; Out: 21; Total: 532</li><li><code>model_version</code>: gemini-2.0-flash-lite</li><li><code>candidates</code>: <details open='true'><summary>candidates[0]</summary><ul><li><code>finish_reason</code>: FinishReason.STOP</li><li><code>content</code>: <ul><li><code>parts</code>: <details open='true'><summary>parts[0]</summary><ul><li><code>text</code>: The checksum of the username \"Miko\" is 96025545.\n",
       "</li></ul></details></li><li><code>role</code>: model</li></ul></li><li><code>avg_logprobs</code>: -0.005351128322737557</li></ul></details></li><li><code>automatic_function_calling_history</code>: </li></ul></details>"
      ],
      "text/plain": [
       "GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='The checksum of the username \"Miko\" is 96025545.\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=-0.005351128322737557, finish_reason=<FinishReason.STOP: 'STOP'>, grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], create_time=None, response_id=None, model_version='gemini-2.0-flash-lite', prompt_feedback=None, usage_metadata=Cached: 0; In: 511; Out: 21; Total: 532, automatic_function_calling_history=[], parsed=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.toolloop(\"Use it to get the checksum of the username of this session.\",  tools=chat._tools, sp=sp, temp=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20715f4e-4dda-4cbe-bf45-66462329f612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
