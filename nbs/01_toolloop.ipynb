{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95f5802f-f203-4455-9de6-f72f43040aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp toolloop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d3ef65-b77e-4937-b78f-70fc3b40c61f",
   "metadata": {},
   "source": [
    "# Tool Loop\n",
    "\n",
    "The code for Claudette's tool loop should essentially work as is. We'll replicate the whole original notebook just to make sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b8eb7ee4-91a4-4f58-942a-ecd3d78eb1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from gaspare.core import *\n",
    "from fastcore.utils import *\n",
    "from fastcore.meta import delegates\n",
    "\n",
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8f9f6dd6-1216-4776-90f2-8910fc883cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemini-2.0-flash'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models[2]\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cb0680-f3bd-4fba-83b5-4d72ad22e988",
   "metadata": {},
   "source": [
    "Let's use the [same example](https://github.com/anthropics/anthropic-cookbook/blob/main/tool_use/customer_service_agent.ipynb) from Claudette's documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c36025e7-18aa-48ad-a313-0badce6ea107",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = {\n",
    "    \"O1\": dict(id=\"O1\", product=\"Widget A\", quantity=2, price=19.99, status=\"Shipped\"),\n",
    "    \"O2\": dict(id=\"O2\", product=\"Gadget B\", quantity=1, price=49.99, status=\"Processing\"),\n",
    "    \"O3\": dict(id=\"O3\", product=\"Gadget B\", quantity=2, price=49.99, status=\"Shipped\")}\n",
    "\n",
    "customers = {\n",
    "    \"C1\": dict(name=\"John Doe\", email=\"john@example.com\", phone=\"123-456-7890\",\n",
    "               orders=[orders['O1'], orders['O2']]),\n",
    "    \"C2\": dict(name=\"Jane Smith\", email=\"jane@example.com\", phone=\"987-654-3210\",\n",
    "               orders=[orders['O3']])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8887b19-fd70-4d9f-9089-8278445f6a34",
   "metadata": {},
   "source": [
    "As with Claudette, we do not have to create the JSON schema manually. We can use docments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b544f2ad-0e09-4466-9ae2-ada005227996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_customer_info(\n",
    "    customer_id:str # ID of the customer\n",
    "): # Customer's name, email, phone number, and list of orders\n",
    "    \"Retrieves a customer's information and their orders based on the customer ID\"\n",
    "    print(f'- Retrieving customer {customer_id}')\n",
    "    return customers.get(customer_id, \"Customer not found\")\n",
    "\n",
    "def get_order_details(\n",
    "    order_id:str # ID of the order\n",
    "): # Order's ID, product name, quantity, price, and order status\n",
    "    \"Retrieves the details of a specific order based on the order ID\"\n",
    "    print(f'- Retrieving order {order_id}')\n",
    "    return orders.get(order_id, \"Order not found\")\n",
    "\n",
    "def cancel_order(\n",
    "    order_id:str # ID of the order to cancel\n",
    ")->bool: # True if the cancellation is successful\n",
    "    \"Cancels an order based on the provided order ID\"\n",
    "    print(f'- Cancelling order {order_id}')\n",
    "    if order_id not in orders: return False\n",
    "    orders[order_id]['status'] = 'Cancelled'\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a7fd0a-93b0-4c76-b8c1-1b40025642e7",
   "metadata": {},
   "source": [
    "We are ready to go. The main difference here is that we don't assign the tools to the chat itself, since otherwise Gemini becomes too eager to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45ddf3f7-a1c7-4178-bf34-03c9ac2345b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_customer_info, get_order_details, cancel_order]\n",
    "chat = Chat(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac4935c5-4504-4e0d-ba22-4879024e5703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Retrieving customer C1\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<ul><li><code>get_customer_info(customer_id=C1)</code></li></ul>\n",
       "<details><ul><li><code>candidates</code>: <details open='true'><summary>candidates[0]</summary><ul><li><code>finish_reason</code>: FinishReason.STOP</li><li><code>content</code>: <ul><li><code>parts</code>: <details open='true'><summary>parts[0]</summary><ul><li><code>function_call</code>: <ul><li><code>args</code>: <ul><li><b>customer_id</b>: C1</li></ul></li><li><code>name</code>: get_customer_info</li></ul></li></ul></details></li><li><code>role</code>: model</li></ul></li><li><code>avg_logprobs</code>: -0.00010187082225456833</li></ul></details></li><li><code>model_version</code>: gemini-2.0-flash</li><li><code>usage_metadata</code>: Cached: 0; In: 147; Out: 10; Total: 157</li><li><code>automatic_function_calling_history</code>: </li></ul></details>"
      ],
      "text/plain": [
       "GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'customer_id': 'C1'}, name='get_customer_info'), function_response=None, inline_data=None, text=None)], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=-0.00010187082225456833, finish_reason=<FinishReason.STOP: 'STOP'>, grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], create_time=None, response_id=None, model_version='gemini-2.0-flash', prompt_feedback=None, usage_metadata=Cached: 0; In: 147; Out: 10; Total: 157, automatic_function_calling_history=[], parsed=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = chat('Can you tell me the email address for customer C1?', tools=tools, use_afc=False)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8042fc83-48b5-4d8a-94f8-f17623088619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The email address for customer C1 is john@example.com.<br />\n",
       "<details><ul><li><code>candidates</code>: <details open='true'><summary>candidates[0]</summary><ul><li><code>finish_reason</code>: FinishReason.STOP</li><li><code>content</code>: <ul><li><code>parts</code>: <details open='true'><summary>parts[0]</summary><ul><li><code>text</code>: The email address for customer C1 is john@example.com.\n",
       "</li></ul></details></li><li><code>role</code>: model</li></ul></li><li><code>avg_logprobs</code>: -2.561106230132282e-05</li></ul></details></li><li><code>model_version</code>: gemini-2.0-flash</li><li><code>usage_metadata</code>: Cached: 0; In: 71; Out: 15; Total: 86</li><li><code>automatic_function_calling_history</code>: </li></ul></details>"
      ],
      "text/plain": [
       "GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='The email address for customer C1 is john@example.com.\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=-2.561106230132282e-05, finish_reason=<FinishReason.STOP: 'STOP'>, grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], create_time=None, response_id=None, model_version='gemini-2.0-flash', prompt_feedback=None, usage_metadata=Cached: 0; In: 71; Out: 15; Total: 86, automatic_function_calling_history=[], parsed=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c9d35f5c-2842-4411-95f7-cd5dd66e7264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are some ideas for things to do with your dog on a rainy day:<br /><br />*   **Indoor games:** Play fetch in a hallway, try hide-and-seek with treats, or work on some new training tricks.<br />*   **Puzzle toys:** These can keep your dog mentally stimulated and entertained.<br />*   **Grooming session:** Rainy days are a good time to brush your dog, trim their nails, or clean their ears.<br />*   **Movie time:** Cuddle up on the couch and watch a dog-friendly movie together.<br />*   **Visit a pet-friendly store:** Many pet supply stores allow dogs, so you can take a trip to pick out a new toy or treat.<br />*   **Indoor playdate:** If your dog gets along with other dogs, invite a friend over for an indoor playdate.<br />*   **Dog-friendly cafe:** Some cafes allow dogs inside, so you can enjoy a coffee while your dog relaxes by your side.<br />*   **Short, leashed walks:** If your dog needs to go outside, take them for a short, leashed walk in a covered area or put them in a raincoat. Dry them off thoroughly when you get back.<br />\n",
       "<details><ul><li><code>candidates</code>: <details open='true'><summary>candidates[0]</summary><ul><li><code>finish_reason</code>: FinishReason.STOP</li><li><code>content</code>: <ul><li><code>parts</code>: <details open='true'><summary>parts[0]</summary><ul><li><code>text</code>: Here are some ideas for things to do with your dog on a rainy day:\n",
       "\n",
       "*   **Indoor games:** Play fetch in a hallway, try hide-and-seek with treats, or work on some new training tricks.\n",
       "*   **Puzzle toys:** These can keep your dog mentally stimulated and entertained.\n",
       "*   **Grooming session:** Rainy days are a good time to brush your dog, trim their nails, or clean their ears.\n",
       "*   **Movie time:** Cuddle up on the couch and watch a dog-friendly movie together.\n",
       "*   **Visit a pet-friendly store:** Many pet supply stores allow dogs, so you can take a trip to pick out a new toy or treat.\n",
       "*   **Indoor playdate:** If your dog gets along with other dogs, invite a friend over for an indoor playdate.\n",
       "*   **Dog-friendly cafe:** Some cafes allow dogs inside, so you can enjoy a coffee while your dog relaxes by your side.\n",
       "*   **Short, leashed walks:** If your dog needs to go outside, take them for a short, leashed walk in a covered area or put them in a raincoat. Dry them off thoroughly when you get back.\n",
       "</li></ul></details></li><li><code>role</code>: model</li></ul></li><li><code>avg_logprobs</code>: -0.12269277954101562</li></ul></details></li><li><code>model_version</code>: gemini-2.0-flash</li><li><code>usage_metadata</code>: Cached: 0; In: 199; Out: 250; Total: 449</li><li><code>automatic_function_calling_history</code>: </li></ul></details>"
      ],
      "text/plain": [
       "GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='Here are some ideas for things to do with your dog on a rainy day:\\n\\n*   **Indoor games:** Play fetch in a hallway, try hide-and-seek with treats, or work on some new training tricks.\\n*   **Puzzle toys:** These can keep your dog mentally stimulated and entertained.\\n*   **Grooming session:** Rainy days are a good time to brush your dog, trim their nails, or clean their ears.\\n*   **Movie time:** Cuddle up on the couch and watch a dog-friendly movie together.\\n*   **Visit a pet-friendly store:** Many pet supply stores allow dogs, so you can take a trip to pick out a new toy or treat.\\n*   **Indoor playdate:** If your dog gets along with other dogs, invite a friend over for an indoor playdate.\\n*   **Dog-friendly cafe:** Some cafes allow dogs inside, so you can enjoy a coffee while your dog relaxes by your side.\\n*   **Short, leashed walks:** If your dog needs to go outside, take them for a short, leashed walk in a covered area or put them in a raincoat. Dry them off thoroughly when you get back.\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=-0.12269277954101562, finish_reason=<FinishReason.STOP: 'STOP'>, grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], create_time=None, response_id=None, model_version='gemini-2.0-flash', prompt_feedback=None, usage_metadata=Cached: 0; In: 199; Out: 250; Total: 449, automatic_function_calling_history=[], parsed=None)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = \"\"\"You will be provided with tools, but don't limit your answer to those tools.\n",
    "If the user query is related to some of the tools you have access to come up with a sequence of actions to achieve the goal and **execute the plan immediately**.\n",
    "\n",
    "If the user query is unrelated to the tools you have access to, answer the query using your own knowledge.\"\"\"\n",
    "\n",
    "chat = Chat(model)\n",
    "r = chat('Cancel all orders for customer C1', tools=tools, use_afc=False, sp=sp, temp=0.6)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "54106d77-417a-481f-9698-c286945f2227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Retrieving customer C1\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<ul><li><code>get_customer_info(customer_id=C1)</code></li></ul>\n",
       "<details><ul><li><code>candidates</code>: <details open='true'><summary>candidates[0]</summary><ul><li><code>finish_reason</code>: FinishReason.STOP</li><li><code>content</code>: <ul><li><code>parts</code>: <details open='true'><summary>parts[0]</summary><ul><li><code>function_call</code>: <ul><li><code>args</code>: <ul><li><b>customer_id</b>: C1</li></ul></li><li><code>name</code>: get_customer_info</li></ul></li></ul></details></li><li><code>role</code>: model</li></ul></li><li><code>avg_logprobs</code>: -0.09488453269004822</li></ul></details></li><li><code>model_version</code>: gemini-2.0-flash</li><li><code>usage_metadata</code>: Cached: 0; In: 848; Out: 10; Total: 858</li><li><code>automatic_function_calling_history</code>: </li></ul></details>"
      ],
      "text/plain": [
       "GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'customer_id': 'C1'}, name='get_customer_info'), function_response=None, inline_data=None, text=None)], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=-0.09488453269004822, finish_reason=<FinishReason.STOP: 'STOP'>, grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], create_time=None, response_id=None, model_version='gemini-2.0-flash', prompt_feedback=None, usage_metadata=Cached: 0; In: 848; Out: 10; Total: 858, automatic_function_calling_history=[], parsed=None)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = chat('Cancel all orders for customer C1', tools=tools, use_afc=False, sp=sp, temp=0.6)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0c44ad55-b87e-463f-b924-56ae2e35315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "@patch\n",
    "@delegates(genai.chats.Chat.__call__)\n",
    "def toolloop(self:genai.chats.Chat,\n",
    "             pr, # Prompt to pass to Gemini\n",
    "             max_steps=10, # Maximum number of tool requests to loop through\n",
    "             trace_func:Optional[callable]=None, # Function to trace tool use steps (e.g `print`)\n",
    "             cont_func:Optional[callable]=noop, # Function that stops loop if returns False\n",
    "             **kwargs):\n",
    "    \"Add prompt `pr` to dialog and get a response from Gemini, automatically following up with `tool_use` messages\"\n",
    "    n_msgs = len(self.h)\n",
    "    kwargs[\"use_afc\"] = False\n",
    "    r = self(pr, **kwargs)\n",
    "    for i in range(max_steps):\n",
    "        if not r.function_calls:break\n",
    "        if trace_func: trace_func(self.h[n_msgs:]); n_msgs = len(self.h)\n",
    "        r = self(**kwargs)\n",
    "        if not (cont_func or noop)(self.h[-2]): break\n",
    "    if trace_func: trace_func(self.h[n_msgs:])\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6d1f588c-a5e0-469e-a1a7-47bbde5b637f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I do not have the ability to retrieve a customer's email address with the available tools. I can only retrieve customer information and their orders based on the customer ID. Would you like me to get the customer information for customer C1?<br />\n",
       "<details><ul><li><code>candidates</code>: <details open='true'><summary>candidates[0]</summary><ul><li><code>finish_reason</code>: FinishReason.STOP</li><li><code>content</code>: <ul><li><code>parts</code>: <details open='true'><summary>parts[0]</summary><ul><li><code>text</code>: I do not have the ability to retrieve a customer's email address with the available tools. I can only retrieve customer information and their orders based on the customer ID. Would you like me to get the customer information for customer C1?\n",
       "</li></ul></details></li><li><code>role</code>: model</li></ul></li><li><code>avg_logprobs</code>: -0.05460302197203344</li></ul></details></li><li><code>model_version</code>: gemini-2.0-flash</li><li><code>usage_metadata</code>: Cached: 0; In: 219; Out: 49; Total: 268</li><li><code>automatic_function_calling_history</code>: </li></ul></details>"
      ],
      "text/plain": [
       "GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"I do not have the ability to retrieve a customer's email address with the available tools. I can only retrieve customer information and their orders based on the customer ID. Would you like me to get the customer information for customer C1?\\n\")], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=-0.05460302197203344, finish_reason=<FinishReason.STOP: 'STOP'>, grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], create_time=None, response_id=None, model_version='gemini-2.0-flash', prompt_feedback=None, usage_metadata=Cached: 0; In: 219; Out: 49; Total: 268, automatic_function_calling_history=[], parsed=None)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat(model)\n",
    "r = chat.toolloop('Tell me the email address for customer C1', tools=tools, sp=sp, temp)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cfec47d8-9832-4cfa-acb5-98d86474a19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Retrieving customer C1\n",
      "\n",
      "Message 1, Part 1:\n",
      "\n",
      "* Text *: Come up with a plan to cancel all orders for customer C1 for me.\n",
      "\n",
      "\n",
      "Message 2, Part 1:\n",
      "\n",
      "* Function Call *: id=None args={'customer_id': 'C1'} name='get_customer_info'\n",
      "\n",
      "- Cancelling order O2\n",
      "\n",
      "Message 1, Part 1:\n",
      "\n",
      "* Function Response *: {'name': 'John Doe', 'email': 'john@example.com', 'phone': '123-456-7890', 'orders': [{'id': 'O1', 'product': 'Widget A', 'quantity': 2, 'price': 19.99, 'status': 'Shipped'}, {'id': 'O2', 'product': 'Gadget B', 'quantity': 1, 'price': 49.99, 'status': 'Cancelled'}]}\n",
      "\n",
      "\n",
      "Message 2, Part 1:\n",
      "\n",
      "* Text *: OK. John Doe has two orders, O1 and O2. O1 is already shipped so I cannot cancel it. I will cancel order O2 for you.\n",
      "\n",
      "\n",
      "\n",
      "Message 2, Part 2:\n",
      "\n",
      "* Function Call *: id=None args={'order_id': 'O2'} name='cancel_order'\n",
      "\n",
      "\n",
      "Message 1, Part 1:\n",
      "\n",
      "* Function Response *: True\n",
      "\n",
      "\n",
      "Message 2, Part 1:\n",
      "\n",
      "* Text *: OK. I have cancelled order O2 for John Doe.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "OK. I have cancelled order O2 for John Doe.<br />\n",
       "<details><ul><li><code>model_version</code>: gemini-2.0-flash</li><li><code>usage_metadata</code>: Cached: 0; In: 257; Out: 13; Total: 270</li><li><code>candidates</code>: <details open='true'><summary>candidates[0]</summary><ul><li><code>avg_logprobs</code>: -0.025467858864710882</li><li><code>finish_reason</code>: FinishReason.STOP</li><li><code>content</code>: <ul><li><code>role</code>: model</li><li><code>parts</code>: <details open='true'><summary>parts[0]</summary><ul><li><code>text</code>: OK. I have cancelled order O2 for John Doe.\n",
       "</li></ul></details></li></ul></li></ul></details></li><li><code>automatic_function_calling_history</code>: </li></ul></details>"
      ],
      "text/plain": [
       "GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='OK. I have cancelled order O2 for John Doe.\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=-0.025467858864710882, finish_reason=<FinishReason.STOP: 'STOP'>, grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], create_time=None, response_id=None, model_version='gemini-2.0-flash', prompt_feedback=None, usage_metadata=Cached: 0; In: 257; Out: 13; Total: 270, automatic_function_calling_history=[], parsed=None)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_msgs(msgs):\n",
    "    for n, m in enumerate(msgs):\n",
    "        for i, part in enumerate(m.parts):\n",
    "            print(f\"\\nMessage {n+1}, Part {i + 1}:\\n\")\n",
    "            c = \"* Text *: \" + part.text if part.text  else \"\" \n",
    "            c += \"* Function Call *: \" + str(part.function_call) if part.function_call else \"\"\n",
    "            c += \"* Function Response *: \" + str(part.function_response.response['result']) if part.function_response else \"\"\n",
    "            print(c)\n",
    "            print()\n",
    "\n",
    "chat = Chat(model)\n",
    "r = chat.toolloop('Come up with a plan to cancel all orders for customer C1 for me.', tools=tools, trace_func=print_msgs)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e849776-9d40-4efc-90c1-f76670b0da69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
